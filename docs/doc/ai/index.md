# Inteligencia Artificial y LLMs

Esta secci贸n cubre el uso de Inteligencia Artificial, espec铆ficamente Large Language Models (LLMs), en entornos DevOps e infraestructura.

##  驴Qu茅 encontrar谩s aqu铆?

- **Fundamentos de LLMs**: Arquitectura, conceptos b谩sicos y casos de uso en DevOps
- **Herramientas locales**: Ollama, LM Studio, LLaMA.cpp para ejecutar modelos localmente
- **Integraci贸n con infraestructura**: Despliegue en Kubernetes, storage optimizado, networking
- **Metodolog铆as de testing**: Benchmarks, evaluaci贸n y prompt engineering
- **Casos pr谩cticos**: Chatbots, an谩lisis de logs, automatizaci贸n IaC

##  Inicio r谩pido

Si eres nuevo en LLMs, comienza con:
1. [Introducci贸n a LLMs](llms_fundamentals.md) - Conceptos b谩sicos
2. [Ollama: primeros pasos](ollama_basics.md) - Tu primera instalaci贸n
3. [Evaluaci贸n de modelos](model_evaluation.md) - C贸mo medir rendimiento

##  Contenido principal

### Fundamentos
- [Arquitectura de Transformers](transformers_architecture.md)
- [Open-source vs Proprietary](open_source_vs_proprietary.md)
- [Casos de uso en DevOps](llms_devops_use_cases.md)

### Herramientas locales
- [Ollama](ollama_basics.md)
- [LM Studio](lm_studio_guide.md)
- [LLaMA.cpp](llama_cpp_optimization.md)

### Integraci贸n con infraestructura
- [LLMs en Kubernetes](llms_kubernetes_deployment.md)
- [Storage para modelos](ai_storage_solutions.md)
- [Networking para IA](ai_networking.md)

### Testing y evaluaci贸n
- [Benchmarks est谩ndar](model_benchmarks.md)
- [Testing de latencia](latency_throughput_testing.md)
- [Prompt engineering](prompt_engineering_basics.md)

##  Enlaces relacionados

- [Documentaci贸n de Ollama](https://github.com/ollama/ollama)
- [LM Studio](https://lmstudio.ai/)
- [LLaMA.cpp](https://github.com/ggerganov/llama.cpp)
- [vLLM](https://github.com/vllm-project/vllm)