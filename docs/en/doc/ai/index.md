# Artificial Intelligence and LLMs

This section covers the use of Artificial Intelligence, specifically Large Language Models (LLMs), in DevOps and infrastructure environments.

## ðŸ¤– What you'll find here

- **LLM Fundamentals**: Architecture, basic concepts and DevOps use cases
- **Local tools**: Ollama, LM Studio, LLaMA.cpp for running models locally
- **Infrastructure integration**: Kubernetes deployment, optimized storage, networking
- **Testing methodologies**: Benchmarks, evaluation and prompt engineering
- **Practical cases**: Chatbots, log analysis, IaC automation

## ðŸš€ Quick start

If you're new to LLMs, start with:
1. [Introduction to LLMs](llms_fundamentals.md) - Basic concepts
2. [Ollama: getting started](ollama_basics.md) - Your first installation
3. [Model evaluation](model_evaluation.md) - How to measure performance

## ðŸ“š Main content

### Fundamentals
- [Transformers Architecture](transformers_architecture.md)
- [Open-source vs Proprietary](open_source_vs_proprietary.md)
- [DevOps Use Cases](llms_devops_use_cases.md)

### Local tools
- [Ollama](ollama_basics.md)
- [LM Studio](lm_studio_guide.md)
- [LLaMA.cpp](llama_cpp_optimization.md)

### Infrastructure integration
- [LLMs in Kubernetes](llms_kubernetes_deployment.md)
- [Storage for models](ai_storage_solutions.md)
- [AI Networking](ai_networking.md)

### Testing and evaluation
- [Standard benchmarks](model_benchmarks.md)
- [Latency testing](latency_throughput_testing.md)
- [Prompt engineering](prompt_engineering_basics.md)

## ðŸ”— Related links

- [Ollama Documentation](https://github.com/ollama/ollama)
- [LM Studio](https://lmstudio.ai/)
- [LLaMA.cpp](https://github.com/ggerganov/llama.cpp)
- [vLLM](https://github.com/vllm-project/vllm)