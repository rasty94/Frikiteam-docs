---
title: "OptimizaciÃ³n de Modelos LLM"
description: "TÃ©cnicas de optimizaciÃ³n para ejecutar LLMs en hardware limitado: cuantizaciÃ³n, pruning, distilaciÃ³n y estrategias de deployment"
date: 2026-01-25
tags: [ai, llm, optimization, quantization, pruning, distillation]
difficulty: intermediate
estimated_time: "25 min"
category: ai
status: published
prerequisites: ["llms_fundamentals", "ollama_basics"]
---

# OptimizaciÃ³n de Modelos LLM

> **Tiempo de lectura:** 25 minutos | **Dificultad:** Intermedia | **CategorÃ­a:** Inteligencia Artificial

## Resumen

La optimizaciÃ³n de modelos LLM es crucial para ejecutar modelos grandes en hardware limitado. Esta guÃ­a cubre tÃ©cnicas como cuantizaciÃ³n, pruning, distilaciÃ³n y estrategias de deployment que permiten reducir el uso de memoria y mejorar el rendimiento sin sacrificar significativamente la calidad.

## ðŸŽ¯ Por QuÃ© Optimizar Modelos LLM

### Problemas Comunes sin OptimizaciÃ³n

- **Memoria insuficiente:** Modelos de 7B-70B parÃ¡metros requieren 14-140GB+ de RAM/VRAM
- **Velocidad lenta:** Inferencia puede tomar segundos por token
- **Costos elevados:** Hardware GPU caro para deployment
- **Escalabilidad limitada:** Dificultad para servir mÃºltiples usuarios

### Beneficios de la OptimizaciÃ³n

- âœ… **70-90% menos memoria** con cuantizaciÃ³n 4-bit
- âœ… **2-5x mÃ¡s velocidad** de inferencia
- âœ… **Hardware mÃ¡s accesible:** GPUs de gama media o CPUs
- âœ… **Mayor throughput:** MÃ¡s usuarios simultÃ¡neos
- âœ… **Costos reducidos:** Menos hardware necesario

## ðŸ”¢ CuantizaciÃ³n (Quantization)

### Â¿QuÃ© es la CuantizaciÃ³n?

La cuantizaciÃ³n reduce la precisiÃ³n de los pesos del modelo de float32 (4 bytes) a formatos mÃ¡s pequeÃ±os como float16, int8 o int4, manteniendo la funcionalidad.

### Tipos de CuantizaciÃ³n

#### 1. **Post-Training Quantization (PTQ)**
- Se aplica despuÃ©s del entrenamiento
- No requiere re-entrenamiento
- RÃ¡pida y sencilla de implementar

```bash
# Ejemplo con Ollama (GGUF quantization)
ollama pull llama2:7b
ollama pull llama2:7b-q4_0  # VersiÃ³n cuantizada 4-bit
```

#### 2. **Quantization-Aware Training (QAT)**
- Se entrena considerando la cuantizaciÃ³n
- Mejor precisiÃ³n pero mÃ¡s complejo
- Usado en producciÃ³n crÃ­tica

### Niveles de CuantizaciÃ³n

| PrecisiÃ³n | Memoria | Velocidad | Calidad | Uso |
|-----------|---------|-----------|---------|-----|
| FP32 | 100% | 100% | 100% | Entrenamiento |
| FP16 | 50% | ~2x | 99.9% | GPUs modernas |
| INT8 | 25% | ~4x | 98-99% | GPUs, CPUs |
| INT4 | 12.5% | ~8x | 95-97% | CPUs, edge |

### ImplementaciÃ³n PrÃ¡ctica

#### Con Ollama (GGUF)

```bash
# Descargar modelo base
ollama pull llama2:7b

# Crear modelo cuantizado personalizado
cat > Modelfile << EOF
FROM llama2:7b
PARAMETER temperature 0.8
PARAMETER top_p 0.9
QUANTIZE q4_0
EOF

ollama create llama2-custom:q4_0 -f Modelfile
```

#### Con llama.cpp

```bash
# Convertir y cuantizar modelo
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# Convertir PyTorch a GGML
python convert.py --model /path/to/model.bin --type f16

# Cuantizar a diferentes formatos
./quantize /path/to/model-f16.bin /path/to/model-q4_0.bin q4_0
```

## ðŸŒ¿ Pruning (Poda)

### Concepto BÃ¡sico

El pruning elimina conexiones neuronales poco importantes, reduciendo el tamaÃ±o del modelo sin afectar significativamente el rendimiento.

### TÃ©cnicas de Pruning

#### 1. **Weight Pruning**
- Elimina pesos individuales por debajo de un threshold
- Efectivo pero requiere fine-tuning posterior

#### 2. **Structured Pruning**
- Elimina neuronas, capas o atenciÃ³n heads completas
- MÃ¡s eficiente para hardware
- Ejemplos: eliminar 20-30% de atenciÃ³n heads

#### 3. **Dynamic Pruning**
- Ajusta la estructura durante inferencia
- Trade-off entre calidad y velocidad

### Ejemplo PrÃ¡ctico

```python
import torch
from transformers import AutoModelForCausalLM

# Cargar modelo
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")

# Aplicar pruning (simplificado)
def prune_weights(model, threshold=0.1):
    for name, param in model.named_parameters():
        if 'weight' in name:
            mask = torch.abs(param) > threshold
            param.data *= mask.float()

prune_weights(model)
```

## ðŸ§ª DistilaciÃ³n (Knowledge Distillation)

### Â¿QuÃ© es la DistilaciÃ³n?

La distilaciÃ³n transfiere conocimiento de un modelo grande ("teacher") a uno mÃ¡s pequeÃ±o ("student"), manteniendo la mayorÃ­a del rendimiento.

### Proceso de DistilaciÃ³n

1. **Teacher Model:** Modelo grande y preciso
2. **Student Model:** Modelo pequeÃ±o a entrenar
3. **Soft Targets:** El student aprende de las distribuciones de probabilidad del teacher
4. **Fine-tuning:** Ajuste final en datos reales

### Ventajas

- âœ… **Mejor ratio calidad/tamaÃ±o** que cuantizaciÃ³n sola
- âœ… **Modelo mÃ¡s pequeÃ±o** pero inteligente
- âœ… **Transfer learning** efectivo

### Ejemplo con Hugging Face

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Modelo teacher (grande)
teacher = AutoModelForCausalLM.from_pretrained("gpt2-large")

# Modelo student (pequeÃ±o)
student = AutoModelForCausalLM.from_pretrained("gpt2")

# FunciÃ³n de pÃ©rdida de distilaciÃ³n
def distillation_loss(student_logits, teacher_logits, temperature=2.0):
    soft_targets = torch.softmax(teacher_logits / temperature, dim=-1)
    student_soft = torch.log_softmax(student_logits / temperature, dim=-1)
    return torch.nn.functional.kl_div(student_soft, soft_targets, reduction='batchmean')
```

## ðŸš€ Estrategias de Deployment

### 1. **CPU vs GPU Optimization**

#### Optimizaciones para CPU
```bash
# Usar llama.cpp con optimizaciones CPU
./main -m model.bin --threads 8 --ctx-size 2048 -p "Prompt"
```

#### Optimizaciones para GPU
```python
# Con vLLM
from vllm import LLM

llm = LLM(
    model="microsoft/DialoGPT-large",
    tensor_parallel_size=2,  # Multi-GPU
    gpu_memory_utilization=0.9,
    max_model_len=4096
)
```

### 2. **Batch Processing**

```python
# Procesamiento por lotes para mayor throughput
prompts = ["Prompt 1", "Prompt 2", "Prompt 3"]

# Inferencia por lotes
outputs = llm.generate(prompts, max_tokens=100)
```

### 3. **Model Caching y Prefilling**

```python
# Cache de KV para prompts comunes
from transformers import Cache

cache = Cache()
model = AutoModelForCausalLM.from_pretrained("gpt2", use_cache=True)

# Primera inferencia crea cache
output1 = model.generate("Common prefix", use_cache=True)

# Inferencias posteriores reutilizan cache
output2 = model.generate("Common prefix + continuation", past_key_values=output1.past_key_values)
```

## ðŸ“Š Benchmarks y Comparativas

### Resultados TÃ­picos de OptimizaciÃ³n

| TÃ©cnica | Modelo | TamaÃ±o Original | TamaÃ±o Optimizado | DegradaciÃ³n | Velocidad |
|---------|--------|-----------------|-------------------|-------------|-----------|
| FP16 | LLaMA-7B | 13GB | 6.5GB | ~0% | 2x |
| INT8 | LLaMA-7B | 13GB | 3.5GB | 1-2% | 4x |
| INT4 | LLaMA-7B | 13GB | 1.8GB | 3-5% | 8x |
| Pruning 20% | GPT-2 | 1.5GB | 1.2GB | 2-3% | 1.5x |
| DistilaciÃ³n | BERT Largeâ†’Base | 340Mâ†’110M params | 67% menos | 3-5% | 2x |

### Hardware Recommendations

| Hardware | Modelo MÃ¡ximo | TÃ©cnica Recomendada |
|----------|----------------|---------------------|
| CPU (16GB RAM) | 7B Q4 | GGUF + llama.cpp |
| GPU RTX 3060 (12GB) | 13B FP16 | vLLM + tensor parallel |
| GPU RTX 4080 (16GB) | 30B Q4 | Ollama + cuantizaciÃ³n |
| CPU Server (128GB) | 70B Q4 | llama.cpp + threads |

## ðŸ› ï¸ Herramientas y Frameworks

### Frameworks de OptimizaciÃ³n

#### 1. **GGUF (GPT-Generated Unified Format)**
- Formato de cuantizaciÃ³n de llama.cpp
- Compatible con mÃºltiples plataformas
- Optimizado para CPU y GPU

```bash
# Convertir modelo a GGUF
python convert-hf-to-gguf.py --model /path/to/model --outdir /output/dir
```

#### 2. **AWQ (Activation-aware Weight Quantization)**
- CuantizaciÃ³n que considera activaciones
- Mejor precisiÃ³n que PTQ estÃ¡ndar
- Especialmente bueno para GPUs

#### 3. **GPTQ (GPT Quantization)**
- CuantizaciÃ³n post-entrenamiento
- Mantiene alta calidad
- Compatible con AutoGPTQ

### Herramientas PrÃ¡cticas

```bash
# Benchmark con llama.cpp
./llama-bench -m model.gguf -p 512 -n 128 -t 8

# Perfilado de memoria
python -c "
import torch
from transformers import AutoModel
model = AutoModel.from_pretrained('model')
print(f'Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters')
print(f'Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB')
"
```

## âš ï¸ Consideraciones y Limitaciones

### Trade-offs Importantes

- **PrecisiÃ³n vs Velocidad:** MÃ¡s cuantizaciÃ³n = mÃ¡s velocidad pero menos precisiÃ³n
- **Calidad vs TamaÃ±o:** Modelos mÃ¡s pequeÃ±os pueden perder capacidades especÃ­ficas
- **Hardware vs Software:** Optimizaciones dependen del hardware objetivo

### Errores Comunes

1. **CuantizaciÃ³n excesiva:** INT2/INT3 puede hacer modelos inutilizables
2. **Pruning agresivo:** Eliminar >50% puede causar colapso del rendimiento
3. **Ignorar fine-tuning:** Modelos optimizados necesitan ajuste posterior

### Mejores PrÃ¡cticas

- âœ… **Probar exhaustivamente** despuÃ©s de optimizar
- âœ… **Monitorear calidad** con mÃ©tricas automatizadas
- âœ… **A/B testing** entre versiones optimizadas
- âœ… **Documentar trade-offs** para stakeholders

## ðŸ”— Recursos Adicionales

- [llama.cpp Documentation](https://github.com/ggerganov/llama.cpp)
- [Hugging Face Optimum](https://huggingface.co/docs/optimum/index)
- [vLLM Performance Guide](https://vllm.readthedocs.io/en/latest/performance.html)
- [GPTQ Paper](https://arxiv.org/abs/2210.17323)

## ðŸ“š PrÃ³ximos Pasos

DespuÃ©s de optimizar modelos, considera:

1. **[Chatbots Locales](chatbots_locales.md)** - Construir interfaces conversacionales
2. **[Prompt Engineering](prompt_engineering.md)** - TÃ©cnicas para mejores resultados
3. **[Deployment en ProducciÃ³n](despliegue_kubernetes.md)** - Servir modelos optimizados a escala

---

*Â¿Has optimizado algÃºn modelo LLM? Comparte tus experiencias y mejores prÃ¡cticas en los comentarios.*</content>
<parameter name="filePath">/Users/antoniorodriguez/Desktop/GIT/FrikiTeam/Frikiteam-docs/docs/doc/ai/model_optimization.md